import concurrent.futures as cf
import glob
import io
import os
import time
import warnings
import logging
from pathlib import Path
from typing import List, Literal
import gradio as gr
import requests
from dotenv import load_dotenv
import pymupdf
from bs4 import BeautifulSoup
import ebooklib
from ebooklib import epub

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from prompts import INSTRUCTION_TEMPLATES, get_template, get_all_template_names
from quality_control import DialogueQualityChecker, validate_dialogue_structure, suggest_improvements
from content_planner import ContentPlanner, SmartContentSplitter, create_adaptive_prompts

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¿½ç•¥ ebooklib çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='ebooklib.epub')
warnings.filterwarnings('ignore', category=FutureWarning, module='ebooklib.epub')

load_dotenv()

# åˆå§‹åŒ–å“è³ªæ§åˆ¶å’Œå…§å®¹è¦åŠƒå™¨
quality_checker = DialogueQualityChecker()
content_planner = ContentPlanner()
content_splitter = SmartContentSplitter()


def fetch_models(api_key, api_base=None):
    """
    Fetch the list of models from the given API base.
    """
    base_url = api_base.rstrip("/") + "/models" if api_base else "https://api.openai.com/v1/models"
    headers = {"Authorization": f"Bearer {api_key}"}
    
    try:
        response = requests.get(base_url, headers=headers)
        if response.status_code == 200:
            models = response.json().get('data', [])
            return [model['id'] for model in models]
        else:
            return [f"Error fetching models: {response.status_code} {response.reason}"]
    except requests.RequestException as e:
        return [f"Error fetching models: {str(e)}"]


def generate_dialogue_via_requests(
    pdf_text: str,
    intro_instructions: str,
    text_instructions: str,
    scratch_pad_instructions: str,
    prelude_dialog: str,
    podcast_dialog_instructions: str,
    model: str,
    llm_api_key: str,
    api_base: str,
    edited_transcript: str = None,
    user_feedback: str = None,
    num_parts: int = 3,
    max_input_length: int = 1000000,
    max_output_tokens: int = 65536,
    progress_callback=None,
    template_type: str = "podcast"
) -> str:
    """
    Generate dialogue by making a direct request to the LLM API.
    ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬ï¼Œæš«æ™‚ä¸ä½¿ç”¨è¤‡é›œçš„å…§å®¹è¦åŠƒ
    """
    logger.info(f"æº–å‚™ç”Ÿæˆå°è©±ï¼Œä½¿ç”¨æ¨¡å‹: {model}")
    
    # é™åˆ¶è¼¸å…¥æ–‡æœ¬é•·åº¦
    original_length = len(pdf_text)
    if len(pdf_text) > max_input_length:
        pdf_text = pdf_text[:max_input_length]
        logger.info(f"è¼¸å…¥æ–‡æœ¬å·²æˆªæ–·: {original_length} -> {max_input_length} å­—ç¬¦")
    
    logger.info(f"è¼¸å…¥æ–‡æœ¬é•·åº¦: {len(pdf_text)} å­—ç¬¦")
    
    # ä½¿ç”¨ç°¡åŒ–çš„æ¨¡æ¿åŒ–æç¤ºè©
    base_prompt = podcast_dialog_instructions.format(content=pdf_text)
    
    # å¦‚æœæœ‰è‡ªå®šç¾©æç¤ºè©æˆ–ç·¨è¼¯éçš„æ–‡ç¨¿ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
    if user_feedback:
        base_prompt += f"\n\nã€é¡å¤–è¦æ±‚ã€‘\n{user_feedback}"
    
    if edited_transcript:
        base_prompt += f"\n\nã€åƒè€ƒæ–‡ç¨¿ã€‘\n{edited_transcript}"

    headers = {
        "Authorization": f"Bearer {llm_api_key}",
        "Content-Type": "application/json"
    }

    base_url = api_base.rstrip("/")
    url = f"{base_url}/chat/completions"
    logger.info(f"æº–å‚™ç™¼é€è«‹æ±‚åˆ° API: {url}")
    
    if progress_callback:
        progress_callback("æ­£åœ¨ç™¼é€è«‹æ±‚åˆ° LLM API...")

    # é‡è©¦åƒæ•¸
    max_retries = 5
    retry_delay = 5

    # ä½¿ç”¨å¯èª¿æ•´çš„è¼¸å‡º token é™åˆ¶
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": base_prompt
            }
        ],
        "temperature": 0.7,
        "max_tokens": max_output_tokens,  # å¯èª¿æ•´çš„è¼¸å‡º token æ•¸
        "stream": False  # å…ˆä¸ç”¨æµå¼ï¼Œç¢ºä¿ç©©å®šæ€§
    }
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ç™¼é€ API è«‹æ±‚ (å˜—è©¦ {attempt+1}/{max_retries})...")
            if progress_callback:
                progress_callback(f"API è«‹æ±‚ä¸­ (å˜—è©¦ {attempt+1}/{max_retries})...")
                
            response = requests.post(url, headers=headers, json=payload)
            
            # è™•ç†é€Ÿç‡é™åˆ¶éŒ¯èª¤
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', retry_delay))
                logger.warning(f"é€Ÿç‡é™åˆ¶éŒ¯èª¤ (429)ã€‚å°‡åœ¨ {retry_after} ç§’å¾Œé‡è©¦ã€‚å˜—è©¦ {attempt+1}/{max_retries}")
                if progress_callback:
                    progress_callback(f"é€Ÿç‡é™åˆ¶éŒ¯èª¤ (429)ã€‚å°‡åœ¨ {retry_after} ç§’å¾Œé‡è©¦...")
                time.sleep(retry_after)
                retry_delay *= 2
                continue
                
            if response.status_code != 200:
                logger.error(f"API è«‹æ±‚å¤±æ•—: ç‹€æ…‹ç¢¼ {response.status_code}, åŸå› : {response.reason}")
                if progress_callback:
                    progress_callback(f"API éŒ¯èª¤: {response.status_code} {response.reason}")
            
            response.raise_for_status()
            result = response.json()
            generated_content = result['choices'][0]['message']['content']
            
            logger.info("API è«‹æ±‚æˆåŠŸï¼Œå·²æ”¶åˆ°å›æ‡‰")
            if progress_callback:
                progress_callback("å·²æˆåŠŸå¾ LLM ç²å–å›æ‡‰")
            
            # æª¢æŸ¥å…§å®¹æ˜¯å¦è¢«æˆªæ–·ï¼ˆæ›´ç²¾ç¢ºçš„æª¢æŸ¥æ–¹å¼ï¼‰
            content_lines = generated_content.strip().split('\n')
            last_line = content_lines[-1] if content_lines else ""
            
            # æ›´ç²¾ç¢ºçš„æˆªæ–·æª¢æ¸¬
            is_truncated = (
                len(generated_content) < 2000 or  # å…§å®¹å¤ªçŸ­
                not last_line.strip() or  # æœ€å¾Œä¸€è¡Œç‚ºç©º
                (last_line.startswith('speaker-') and len(last_line.split(':', 1)) > 1 and 
                 len(last_line.split(':', 1)[1].strip()) < 10) or  # speaker è¡Œå…§å®¹å¤ªçŸ­
                generated_content.strip().endswith(('åœ¨', 'çš„', 'äº†', 'æ˜¯', 'æœƒ', 'ä½†', 'å› ç‚º', 'æ‰€ä»¥', 'é€™', 'é‚£'))
            )
            
            if is_truncated:
                logger.warning("æª¢æ¸¬åˆ°å…§å®¹å¯èƒ½è¢«æˆªæ–·ï¼Œå˜—è©¦åˆ†æ‰¹ç”Ÿæˆ...")
                if progress_callback:
                    progress_callback("æª¢æ¸¬åˆ°å…§å®¹å¯èƒ½è¢«æˆªæ–·ï¼Œå˜—è©¦åˆ†æ‰¹ç”Ÿæˆ...")
                
                # å¦‚æœå…§å®¹è¢«æˆªæ–·ï¼Œä½¿ç”¨åˆ†æ‰¹ç”Ÿæˆ
                full_content = _generate_in_batches(
                    pdf_text, base_prompt, headers, url, model, num_parts, 
                    progress_callback, max_retries, retry_delay
                )
                if full_content:
                    generated_content = full_content
                    logger.info("ä½¿ç”¨åˆ†æ‰¹ç”ŸæˆæˆåŠŸç²å¾—å®Œæ•´å…§å®¹")
                else:
                    logger.warning("åˆ†æ‰¹ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å…§å®¹")
            
            # **å°å®Œæ•´æ–‡ç¨¿é€²è¡Œå“è³ªæª¢æŸ¥**
            try:
                logger.info("é–‹å§‹é€²è¡Œå°è©±å“è³ªæª¢æŸ¥")
                quality_report = quality_checker.check_dialogue_quality(generated_content, ['speaker-1', 'speaker-2'])
                logger.info(f"å“è³ªæª¢æŸ¥å®Œæˆï¼Œç¸½åˆ†: {quality_report.overall_score:.1f}")
                if progress_callback:
                    progress_callback(f"å“è³ªæª¢æŸ¥å®Œæˆï¼Œåˆ†æ•¸: {quality_report.overall_score:.1f}/100")
            except Exception as e:
                logger.warning(f"å“è³ªæª¢æŸ¥å¤±æ•—: {e}")
            
            return generated_content
            
        except requests.exceptions.RequestException as e:
            error_msg = f"è«‹æ±‚å¤±æ•—: {str(e)}"
            logger.error(error_msg)
            
            if attempt < max_retries - 1:
                retry_msg = f"å°‡åœ¨ {retry_delay} ç§’å¾Œé‡è©¦ã€‚å˜—è©¦ {attempt+1}/{max_retries}"
                logger.info(retry_msg)
                if progress_callback:
                    progress_callback(f"{error_msg} {retry_msg}")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                final_error = f"åœ¨ {max_retries} æ¬¡å˜—è©¦å¾Œå¤±æ•—: {str(e)}"
                logger.error(final_error)
                if progress_callback:
                    progress_callback(final_error)
                return f"Error after {max_retries} attempts: {str(e)}"
    
    return "ç”Ÿæˆå¤±æ•—"


def generate_summary(
    script_content: str,
    summary_type: str,
    model: str,
    llm_api_key: str,
    api_base: str,
    max_output_tokens: int = 4096,
    progress_callback=None
) -> str:
    """
    ç‚ºç”Ÿæˆçš„è…³æœ¬å‰µå»ºæ‘˜è¦
    """
    if not script_content or not script_content.strip():
        return "éŒ¯èª¤ï¼šè«‹å…ˆç”Ÿæˆè…³æœ¬å…§å®¹"
    
    logger.info(f"é–‹å§‹ç”Ÿæˆæ‘˜è¦ï¼Œé¡å‹: {summary_type}")
    
    # å¾ prompts æ¨¡çµ„ç²å–æ‘˜è¦æ¨¡æ¿
    try:
        summary_template = get_template(summary_type)["dialog"]
        prompt = summary_template.format(content=script_content)
    except KeyError:
        return f"éŒ¯èª¤ï¼šæœªæ‰¾åˆ°æ‘˜è¦æ¨¡æ¿ '{summary_type}'"
    
    headers = {
        "Authorization": f"Bearer {llm_api_key}",
        "Content-Type": "application/json"
    }
    
    base_url = api_base.rstrip("/")
    url = f"{base_url}/chat/completions"
    
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.7,
        "max_tokens": max_output_tokens
    }
    
    if progress_callback:
        progress_callback(f"æ­£åœ¨ç”Ÿæˆ{summary_type}æ‘˜è¦...")
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        summary = result['choices'][0]['message']['content']
        
        logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•·åº¦: {len(summary)} å­—ç¬¦")
        if progress_callback:
            progress_callback(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
        
        return summary
        
    except requests.exceptions.RequestException as e:
        error_msg = f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}"
        logger.error(error_msg)
        if progress_callback:
            progress_callback(error_msg)
        return error_msg


def _generate_in_batches(pdf_text, base_prompt, headers, url, model, num_parts, progress_callback, max_retries, retry_delay):
    """
    åˆ†æ‰¹ç”Ÿæˆçš„å‚™ç”¨æ©Ÿåˆ¶ï¼Œåªåœ¨å–®æ¬¡ç”Ÿæˆè¢«æˆªæ–·æ™‚ä½¿ç”¨
    """
    try:
        logger.info(f"é–‹å§‹åˆ†æ‰¹ç”Ÿæˆï¼Œå…± {num_parts} å€‹éƒ¨åˆ†")
        
        # ç”Ÿæˆå…§å®¹å¤§ç¶±ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        outline_prompt = f"""
è«‹ç‚ºä»¥ä¸‹å…§å®¹ç”Ÿæˆä¸€å€‹ç°¡æ½”çš„è¨è«–å¤§ç¶±ï¼ŒåŒ…å« {num_parts} å€‹ä¸»è¦éƒ¨åˆ†ï¼š

{pdf_text[:5000]}...

è«‹ç”¨ç¹é«”ä¸­æ–‡åˆ—å‡º {num_parts} å€‹ä¸»è¦è¨è«–ä¸»é¡Œï¼Œæ¯å€‹ä¸»é¡Œä¸€è¡Œã€‚
"""
        
        # ç²å–å¤§ç¶±
        outline_payload = {
            "model": model,
            "messages": [{"role": "user", "content": outline_prompt}],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        outline_response = requests.post(url, headers=headers, json=outline_payload)
        outline = ""
        if outline_response.status_code == 200:
            outline = outline_response.json()['choices'][0]['message']['content']
            logger.info(f"ç²å¾—å…§å®¹å¤§ç¶±: {outline[:100]}...")
        
        # åˆ†æ‰¹ç”Ÿæˆ
        dialogue_parts = []
        context_summary = ""
        
        for part_index in range(num_parts):
            is_first_part = part_index == 0
            is_last_part = part_index == num_parts - 1
            
            if is_first_part:
                part_prompt = f"""
å°‡ä»¥ä¸‹å…§å®¹è½‰æ›æˆæ’­å®¢å°è©±çš„ç¬¬ 1/{num_parts} éƒ¨åˆ†ï¼š

ã€å…§å®¹ä¾†æºã€‘
{pdf_text[:10000]}...

ã€å¤§ç¶±åƒè€ƒã€‘
{outline}

ã€è¦æ±‚ã€‘
- æŒ‰ç…§æ­£å¸¸æ ¼å¼é–‹å ´ï¼šspeaker-1: æ­¡è¿æ”¶è½ David888 Podcastï¼Œæˆ‘æ˜¯ David...
- speaker-2 é¦–æ¬¡ç™¼è¨€æ™‚è‡ªæˆ‘ä»‹ç´¹ç‚º Cordelia
- è¨è«–å‰é¢çš„ä¸»é¡Œ
- **ä¸è¦çµæŸå°è©±**ï¼Œåœ¨ä¸€å€‹é–‹æ”¾çš„è¨è«–é»åœæ­¢
- å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ ¼å¼ç‚º speaker-1: å’Œ speaker-2:
"""
            elif is_last_part:
                part_prompt = f"""
å»¶çºŒä¹‹å‰çš„æ’­å®¢å°è©±ï¼Œé€™æ˜¯ç¬¬ {part_index+1}/{num_parts} éƒ¨åˆ†ï¼ˆæœ€å¾Œä¸€éƒ¨åˆ†ï¼‰ã€‚

ã€å‰æ–‡æ‘˜è¦ã€‘
{context_summary[-3000:]}

ã€å¤§ç¶±åƒè€ƒã€‘
{outline}

ã€å…§å®¹ä¾†æºã€‘
{pdf_text}

è«‹ï¼š
1. **ä¸è¦é‡è¤‡é–‹å ´**ï¼Œç›´æ¥ç¹¼çºŒå‰é¢çš„å°è©±
2. å®Œæˆå‰©é¤˜ä¸»é¡Œçš„è¨è«–
3. **è‡ªç„¶åœ°çµæŸå°è©±**ï¼ŒåŒ…å«ç¸½çµå’Œå‘Šåˆ¥

**å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ ¼å¼ç‚º speaker-1: å’Œ speaker-2:**
"""
            else:
                part_prompt = f"""
å»¶çºŒä¹‹å‰çš„æ’­å®¢å°è©±ï¼Œé€™æ˜¯ç¬¬ {part_index+1}/{num_parts} éƒ¨åˆ†ï¼ˆä¸­é–“éƒ¨åˆ†ï¼‰ã€‚

ã€å‰æ–‡æ‘˜è¦ã€‘
{context_summary[-3000:]}

ã€å¤§ç¶±åƒè€ƒã€‘
{outline}

ã€å…§å®¹ä¾†æºã€‘
{pdf_text}

è«‹ï¼š
1. **ä¸è¦é‡è¤‡é–‹å ´**ï¼Œç›´æ¥ç¹¼çºŒå‰é¢çš„å°è©±
2. è¨è«–ç›¸æ‡‰çš„ä¸»é¡Œ
3. **ä¸è¦çµæŸå°è©±**ï¼Œåœ¨ä¸€å€‹é–‹æ”¾çš„è¨è«–é»åœæ­¢

**å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ ¼å¼ç‚º speaker-1: å’Œ speaker-2:**
"""
            
            # ç”Ÿæˆç•¶å‰éƒ¨åˆ†
            part_payload = {
                "model": model,
                "messages": [{"role": "user", "content": part_prompt}],
                "temperature": 0.7,
                "max_tokens": 8192
            }
            
            for attempt in range(max_retries):
                try:
                    if progress_callback:
                        progress_callback(f"ç”Ÿæˆç¬¬ {part_index+1}/{num_parts} éƒ¨åˆ† (å˜—è©¦ {attempt+1})...")
                    
                    part_response = requests.post(url, headers=headers, json=part_payload)
                    part_response.raise_for_status()
                    
                    current_part = part_response.json()['choices'][0]['message']['content']
                    dialogue_parts.append(current_part)
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡æ‘˜è¦
                    if context_summary:
                        context_summary += "\n\n" + current_part
                    else:
                        context_summary = current_part
                    
                    logger.info(f"å®Œæˆç¬¬ {part_index+1}/{num_parts} éƒ¨åˆ†")
                    break
                    
                except Exception as e:
                    logger.error(f"ç”Ÿæˆç¬¬ {part_index+1} éƒ¨åˆ†å¤±æ•—: {e}")
                    if attempt == max_retries - 1:
                        return None
                    time.sleep(retry_delay)
        
        # åˆä½µæ‰€æœ‰éƒ¨åˆ†
        full_dialogue = "\n\n".join(dialogue_parts)
        logger.info(f"åˆ†æ‰¹ç”Ÿæˆå®Œæˆï¼Œç¸½é•·åº¦: {len(full_dialogue)} å­—ç¬¦")
        
        return full_dialogue
        
    except Exception as e:
        logger.error(f"åˆ†æ‰¹ç”Ÿæˆå¤±æ•—: {e}")
        return None


def validate_and_generate_script(
    files,
    openai_api_key,
    text_model,
    api_base_value,
    intro_instructions,
    text_instructions,
    scratch_pad_instructions,
    prelude_dialog,
    podcast_dialog_instructions,
    edited_transcript,
    user_feedback,
    num_parts=3,
    max_input_length=1000000,
    max_output_tokens=65536,
    progress_callback=None
):
    """é©—è­‰è¼¸å…¥ä¸¦ç”Ÿæˆè…³æœ¬"""
    if not files:
        logger.warning("æœªä¸Šå‚³æ–‡ä»¶")
        if progress_callback:
            progress_callback("éŒ¯èª¤ï¼šè«‹ä¸Šå‚³è‡³å°‘ä¸€å€‹æ–‡ä»¶")
        return None, "è«‹åœ¨ç”Ÿæˆè…³æœ¬å‰ä¸Šå‚³è‡³å°‘ä¸€å€‹æ–‡ä»¶ã€‚"

    try:
        logger.info(f"é–‹å§‹è™•ç† {len(files)} å€‹æ–‡ä»¶")
        if progress_callback:
            progress_callback(f"é–‹å§‹è™•ç† {len(files)} å€‹æ–‡ä»¶...")
        
        # å¾æª”æ¡ˆä¸­æå–æ–‡å­—
        combined_text = ""
        for file in files:
            filename = file.name.lower()
            logger.info(f"è™•ç†æ–‡ä»¶: {filename}")
            if progress_callback:
                progress_callback(f"è™•ç†æ–‡ä»¶: {os.path.basename(filename)}")

            if filename.endswith(".pdf"):
                try:
                    logger.info(f"ä½¿ç”¨ PyMuPDF é–‹å•Ÿ PDF: {filename}")
                    doc = pymupdf.open(file.name)
                    page_count = len(doc)
                    logger.info(f"PDF é æ•¸: {page_count}")
                    
                    for i, page in enumerate(doc):
                        page_text = page.get_text()
                        combined_text += page_text + "\n\n"
                        if i % 10 == 0:
                            logger.debug(f"å·²è™•ç† PDF ç¬¬ {i+1}/{page_count} é ")
                            if progress_callback and page_count > 10:
                                progress_callback(f"è™•ç† PDF: {os.path.basename(filename)} - {i+1}/{page_count} é ")
                    
                    logger.info(f"PDF è™•ç†å®Œæˆ: {filename}")
                    if progress_callback:
                        progress_callback(f"PDF è™•ç†å®Œæˆ: {os.path.basename(filename)}")
                except Exception as e:
                    error_msg = f"PDF è™•ç†éŒ¯èª¤ ({filename}): {str(e)}"
                    logger.error(error_msg)
                    if progress_callback:
                        progress_callback(error_msg)

            elif filename.endswith(".txt"):
                try:
                    logger.info(f"è™•ç†æ–‡æœ¬æ–‡ä»¶: {filename}")
                    with open(file.name, "r", encoding="utf-8", errors="ignore") as f:
                        file_text = f.read()
                        combined_text += file_text + "\n\n"
                        logger.info(f"æ–‡æœ¬æ–‡ä»¶è™•ç†å®Œæˆï¼Œé•·åº¦: {len(file_text)} å­—ç¬¦")
                        if progress_callback:
                            progress_callback(f"æ–‡æœ¬æ–‡ä»¶è™•ç†å®Œæˆ: {os.path.basename(filename)}")
                except Exception as e:
                    error_msg = f"TXT æ–‡ä»¶è™•ç†éŒ¯èª¤ ({filename}): {str(e)}"
                    logger.error(error_msg)
                    if progress_callback:
                        progress_callback(error_msg)

            elif filename.endswith(".epub"):
                try:
                    logger.info(f"è™•ç† EPUB æ–‡ä»¶: {filename}")
                    if progress_callback:
                        progress_callback(f"è™•ç† EPUB æ–‡ä»¶: {os.path.basename(filename)}")
                    
                    book = epub.read_epub(file.name)
                    item_count = len(list(book.get_items()))
                    processed_count = 0
                    
                    for item in book.get_items():
                        if item.get_type() == ebooklib.ITEM_DOCUMENT:
                            try:
                                processed_count += 1
                                soup = BeautifulSoup(item.get_body_content(), 'html.parser')
                                item_text = soup.get_text()
                                combined_text += item_text + "\n\n"
                                logger.debug(f"å·²è™•ç† EPUB é …ç›® {processed_count}/{item_count}")
                                if processed_count % 5 == 0 and progress_callback:
                                    progress_callback(f"è™•ç† EPUB: {os.path.basename(filename)} - {processed_count}/{item_count} é …ç›®")
                            except Exception as e:
                                logger.error(f"EPUB é …ç›®è™•ç†éŒ¯èª¤: {str(e)}")
                    
                    logger.info(f"EPUB è™•ç†å®Œæˆ: {filename}, å…±è™•ç† {processed_count} å€‹é …ç›®")
                    if progress_callback:
                        progress_callback(f"EPUB è™•ç†å®Œæˆ: {os.path.basename(filename)}, å…±è™•ç† {processed_count} å€‹é …ç›®")
                except Exception as e:
                    error_msg = f"EPUB è™•ç†éŒ¯èª¤ ({filename}): {str(e)}"
                    logger.error(error_msg)
                    if progress_callback:
                        progress_callback(error_msg)
            else:
                logger.warning(f"è·³éä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
                if progress_callback:
                    progress_callback(f"è·³éä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {os.path.basename(filename)}")

        text_length = len(combined_text)
        logger.info(f"æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œæˆï¼Œåˆä½µæ–‡æœ¬é•·åº¦: {text_length} å­—ç¬¦")
        if progress_callback:
            progress_callback(f"æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œæˆï¼Œåˆä½µæ–‡æœ¬é•·åº¦: {text_length} å­—ç¬¦")

        # ç”Ÿæˆå°è©±è…³æœ¬
        logger.info("é–‹å§‹ç”Ÿæˆè…³æœ¬...")
        if progress_callback:
            progress_callback("é–‹å§‹ç”Ÿæˆè…³æœ¬ï¼Œæ­£åœ¨ç™¼é€è«‹æ±‚åˆ° LLM API...")
            
        script = generate_dialogue_via_requests(
            pdf_text=combined_text,
            intro_instructions=intro_instructions,
            text_instructions=text_instructions,
            scratch_pad_instructions=scratch_pad_instructions,
            prelude_dialog=prelude_dialog,
            podcast_dialog_instructions=podcast_dialog_instructions,
            model=text_model,
            llm_api_key=openai_api_key,
            api_base=api_base_value,
            edited_transcript=edited_transcript,
            user_feedback=user_feedback,
            num_parts=num_parts,
            max_input_length=max_input_length,
            max_output_tokens=max_output_tokens,
            progress_callback=progress_callback,
            template_type="podcast"
        )

        logger.info("è…³æœ¬ç”Ÿæˆå®Œæˆ")
        if progress_callback:
            progress_callback("è…³æœ¬ç”Ÿæˆå®Œæˆï¼")
        return script, None

    except Exception as e:
        error_msg = f"è…³æœ¬ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        logger.error(error_msg)
        if progress_callback:
            progress_callback(error_msg)
        return None, error_msg


# Gradio ä»‹é¢
with gr.Blocks(title="Script Generator", css="""
    #generate-btn {
        background-color: #FF9800 !important;
        color: white !important;
    }
    #header { text-align: center; margin-bottom: 20px; }
    .error { color: red; }
""") as demo:
    gr.Markdown("# è…³æœ¬ç”Ÿæˆå™¨ | Script Generator (é‡æ§‹ç‰ˆ)", elem_id="header")
    
    with gr.Row():
        with gr.Column(scale=1):
            # è¼¸å…¥å€
            files = gr.Files(
                label="ä¸Šå‚³æª”æ¡ˆ | Upload Files",
                file_types=[".pdf", ".txt", ".epub"],
                file_count="multiple",
                interactive=True
            )
            
            api_base = gr.Textbox(
                label="API Base URL",
                placeholder="https://generativelanguage.googleapis.com/v1beta/openai",
                value="https://generativelanguage.googleapis.com/v1beta/openai"
            )
            
            api_key = gr.Textbox(
                label="LLM API Key",
                type="password"
            )
            
            model_dropdown = gr.Dropdown(
                label="é¸æ“‡æ¨¡å‹ | Select Model",
                choices=[],
                interactive=True
            )
            
            fetch_button = gr.Button("ç²å–æ¨¡å‹åˆ—è¡¨ | Fetch Models")
            
            template_dropdown = gr.Dropdown(
                label="æç¤ºè©æ¨¡æ¿ | Prompt Template",
                choices=get_all_template_names(),
                value="podcast",
                interactive=True
            )
            
            intro_text = gr.Textbox(
                label="ä»‹ç´¹æç¤ºè© | Intro Instructions",
                lines=5,
                value=INSTRUCTION_TEMPLATES["podcast"]["intro"],
                interactive=True,
                visible=False  # éš±è—æ­¤æ¬„ä½
            )
            
            text_instructions = gr.Textbox(
                label="æ–‡æœ¬åˆ†ææç¤ºè© | Text Instructions",
                lines=5,
                value=INSTRUCTION_TEMPLATES["podcast"]["text_instructions"],
                interactive=True,
                visible=False  # éš±è—æ­¤æ¬„ä½
            )
            
            scratch_pad = gr.Textbox(
                label="è…¦åŠ›æ¿€ç›ªæç¤ºè© | Scratch Pad",
                lines=5,
                value=INSTRUCTION_TEMPLATES["podcast"]["scratch_pad"],
                interactive=True,
                visible=False  # éš±è—æ­¤æ¬„ä½
            )
            
            prelude = gr.Textbox(
                label="å‰å°æç¤ºè© | Prelude",
                lines=5,
                value=INSTRUCTION_TEMPLATES["podcast"]["prelude"],
                interactive=True,
                visible=False  # éš±è—æ­¤æ¬„ä½
            )
            
            dialog = gr.Textbox(
                label="ä¸»è¦æç¤ºè© | Main Prompt (é è¦½ç”¨ï¼Œç”±æ¨¡æ¿è‡ªå‹•è¨­å®š)",
                lines=8,
                value=INSTRUCTION_TEMPLATES["podcast"]["dialog"], 
                interactive=True,
                info="é€™æ˜¯ç•¶å‰é¸æ“‡æ¨¡æ¿çš„æç¤ºè©å…§å®¹ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹å‹•ä¿®æ”¹"
            )
            
            custom_prompt = gr.Textbox(
                label="è‡ªå®šç¾©æç¤ºè© | Custom Prompt",
                placeholder="Optional: Enter your custom prompt here",
                lines=5
            )
            
            # æ·»åŠ åˆ†æ‰¹ç”Ÿæˆéƒ¨åˆ†æ•¸é‡çš„æ»‘å‹•æ¢
            num_parts_slider = gr.Slider(
                minimum=1,
                maximum=5,
                value=1,
                step=1,
                label="åˆ†æ‰¹ç”Ÿæˆéƒ¨åˆ†æ•¸é‡ | Number of Generation Parts",
                info="æš«æ™‚è¨­ç‚º1ï¼Œæœªä¾†ç‰ˆæœ¬å°‡æ”¯æ´æ™ºèƒ½åˆ†æ‰¹ç”Ÿæˆ"
            )
            
            # æ·»åŠ æœ€å¤§è¼¸å…¥æ–‡æœ¬é•·åº¦çš„æ»‘å‹•æ¢
            max_input_length_slider = gr.Slider(
                minimum=50000,
                maximum=2000000,
                value=1000000,
                step=50000,
                label="æœ€å¤§è¼¸å…¥æ–‡æœ¬é•·åº¦ | Max Input Text Length",
                info="èª¿æ•´æ¨¡å‹å¯è™•ç†çš„æœ€å¤§è¼¸å…¥æ–‡æœ¬é•·åº¦ï¼ˆå­—ç¬¦æ•¸ï¼‰"
            )
            
            # æ·»åŠ æœ€å¤§è¼¸å‡º token æ•¸çš„æ»‘å‹•æ¢
            max_output_tokens_slider = gr.Slider(
                minimum=1024,
                maximum=131072,
                value=65536,
                step=1024,
                label="æœ€å¤§è¼¸å‡º Token æ•¸ | Max Output Tokens",
                info="èª¿æ•´æ¨¡å‹æœ€å¤§è¼¸å‡º token æ•¸ã€‚Gemini Flash 2.5: 65536, GPT-4: 4096, Claude: 8192"
            )
            
        
        with gr.Column(scale=1):
            # è¼¸å‡ºå€
            generate_button = gr.Button("ç”Ÿæˆè…³æœ¬ | Generate Script", elem_id="generate-btn")
            
            output_text = gr.Textbox(
                label="ç”Ÿæˆçš„è…³æœ¬ | Generated Script",
                lines=20,
                show_copy_button=True
            )
            
            # æ‘˜è¦ç”Ÿæˆå€åŸŸ
            gr.Markdown("### ğŸ“ Podcast æ‘˜è¦ç”Ÿæˆ | Summary Generation")
            
            with gr.Row():
                summary_type_dropdown = gr.Dropdown(
                    label="æ‘˜è¦é¡å‹ | Summary Type",
                    choices=["blog-summary", "intro-summary"],
                    value="intro-summary",
                    interactive=True
                )
                
                generate_summary_button = gr.Button("ç”Ÿæˆæ‘˜è¦ | Generate Summary", size="sm")
            
            summary_output = gr.Textbox(
                label="ç”Ÿæˆçš„æ‘˜è¦ | Generated Summary",
                lines=10,
                show_copy_button=True,
                placeholder="è«‹å…ˆç”Ÿæˆè…³æœ¬ï¼Œç„¶å¾Œé»æ“Šã€Œç”Ÿæˆæ‘˜è¦ã€æŒ‰éˆ•"
            )
            
            error_output = gr.Markdown(
                visible=False,
                elem_classes=["error"]
            )
    
    # äº‹ä»¶è™•ç†
    def handle_model_fetch(key, base):
        logger.info(f"å˜—è©¦å¾ {base} ç²å–æ¨¡å‹åˆ—è¡¨")
        if not key:
            logger.warning("æœªæä¾› API å¯†é‘°")
            return gr.update(choices=[], value=None), gr.update(visible=True, value="éŒ¯èª¤: éœ€è¦ API å¯†é‘°")
        
        models = fetch_models(key, base)
        
        if isinstance(models, list) and models and not models[0].startswith("Error"):
            logger.info(f"æˆåŠŸç²å– {len(models)} å€‹æ¨¡å‹")
            return gr.update(choices=models, value=models[0]), gr.update(visible=False)
        
        error_msg = models[0] if models else "æœªçŸ¥éŒ¯èª¤"
        logger.error(f"ç²å–æ¨¡å‹å¤±æ•—: {error_msg}")
        return gr.update(choices=[], value=None), gr.update(visible=True, value=error_msg)
    
    def update_template(template):
        logger.info(f"åˆ‡æ›æ¨¡æ¿è‡³: {template}")
        try:
            template_data = get_template(template)
            return [
                template_data["intro"],
                template_data["text_instructions"],
                template_data["scratch_pad"],
                template_data["prelude"],
                template_data["dialog"]
            ]
        except KeyError:
            logger.error(f"æ¨¡æ¿ {template} ä¸å­˜åœ¨")
            return ["", "", "", "", ""]
    
    fetch_button.click(
        fn=handle_model_fetch,
        inputs=[api_key, api_base],
        outputs=[model_dropdown, error_output]
    )
    
    template_dropdown.change(
        fn=update_template,
        inputs=[template_dropdown],
        outputs=[intro_text, text_instructions, scratch_pad, prelude, dialog]
    )
    
    def handle_script_generation(*args):
        logger.info("é–‹å§‹ç”Ÿæˆè…³æœ¬")
        script, error = validate_and_generate_script(*args)
        if error:
            logger.error(f"è…³æœ¬ç”Ÿæˆå¤±æ•—: {error}")
            return None, gr.update(visible=True, value=error)
        logger.info("è…³æœ¬ç”ŸæˆæˆåŠŸ")
        return script, gr.update(visible=False)
    
    def handle_summary_generation(script_content, summary_type, api_key_val, model_val, api_base_val, max_tokens_val):
        if not script_content or not script_content.strip():
            return "éŒ¯èª¤ï¼šè«‹å…ˆç”Ÿæˆè…³æœ¬å…§å®¹"
        
        if not api_key_val or not model_val:
            return "éŒ¯èª¤ï¼šè«‹ç¢ºä¿å·²è¨­å®š API é‡‘é‘°å’Œæ¨¡å‹"
        
        logger.info(f"é–‹å§‹ç”Ÿæˆæ‘˜è¦ï¼Œé¡å‹: {summary_type}")
        
        def progress_callback(msg):
            pass  # ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸é¡¯ç¤ºé€²åº¦
        
        summary = generate_summary(
            script_content=script_content,
            summary_type=summary_type,
            model=model_val,
            llm_api_key=api_key_val,
            api_base=api_base_val,
            max_output_tokens=max_tokens_val // 2,  # æ‘˜è¦ä½¿ç”¨è¼ƒå°‘çš„ tokens
            progress_callback=progress_callback
        )
        
        return summary
    
    generate_button.click(
        fn=handle_script_generation,
        inputs=[
            files,
            api_key,
            model_dropdown,
            api_base,
            intro_text,
            text_instructions,
            scratch_pad,
            prelude,
            dialog,
            gr.Textbox(value=""),  # edited_transcript
            custom_prompt,  # user_feedback
            num_parts_slider,  # æ·»åŠ æ»‘å‹•æ¢åƒæ•¸
            max_input_length_slider,  # æ·»åŠ æœ€å¤§è¼¸å…¥æ–‡æœ¬é•·åº¦åƒæ•¸
            max_output_tokens_slider  # æ·»åŠ æœ€å¤§è¼¸å‡º token æ•¸åƒæ•¸
        ],
        outputs=[output_text, error_output]
    )
    
    generate_summary_button.click(
        fn=handle_summary_generation,
        inputs=[
            output_text,  # è…³æœ¬å…§å®¹
            summary_type_dropdown,  # æ‘˜è¦é¡å‹
            api_key,  # API é‡‘é‘°
            model_dropdown,  # æ¨¡å‹
            api_base,  # API åŸºç¤ URL
            max_output_tokens_slider  # æœ€å¤§è¼¸å‡º tokens
        ],
        outputs=[summary_output]
    )


if __name__ == "__main__":
    logger.info("å•Ÿå‹•è…³æœ¬ç”Ÿæˆå™¨æ‡‰ç”¨ (é‡æ§‹ç‰ˆ)")
    demo.launch()